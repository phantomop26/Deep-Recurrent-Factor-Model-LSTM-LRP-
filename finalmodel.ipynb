{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jH_p7TveLwRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "sp500 = pd.read_csv('sp500.csv')\n",
        "symbols = sp500['Symbol'].tolist()\n",
        "\n",
        "start_date = '1990-12-31'\n",
        "end_date   = '2025-04-30'\n",
        "all_tickers = symbols + ['SPY']\n",
        "\n",
        "monthly = yf.download(\n",
        "    tickers=all_tickers,\n",
        "    start=start_date,\n",
        "    end=end_date,\n",
        "    interval='1mo',\n",
        "    group_by='ticker',\n",
        "    auto_adjust=True,\n",
        "    threads=True,\n",
        "    progress=False\n",
        ")\n",
        "\n",
        "available = [sym for sym in symbols if sym in monthly.columns.get_level_values(0)]\n",
        "missing   = sorted(set(symbols) - set(available))\n",
        "if missing:\n",
        "    print(f\"{missing} missing\")\n",
        "\n",
        "price_m = pd.DataFrame({ sym: monthly[sym]['Close'] for sym in available })\n",
        "returns = price_m.pct_change().dropna(how='all')\n",
        "\n",
        "daily = yf.download(\n",
        "    tickers=all_tickers,\n",
        "    start=start_date,\n",
        "    end=end_date,\n",
        "    interval='1d',\n",
        "    group_by='ticker',\n",
        "    auto_adjust=True,\n",
        "    threads=True,\n",
        "    progress=True\n",
        ")\n",
        "\n",
        "price_d = pd.DataFrame({ sym: daily[sym]['Close'] for sym in available })\n",
        "\n",
        "spy_ret = monthly['SPY']['Close'].pct_change().dropna()\n",
        "\n",
        "window = 60\n",
        "risk = {}\n",
        "for sym in available:\n",
        "    r = returns[sym]\n",
        "    risk[(sym,'60VOL')] = r.rolling(window).std()\n",
        "    cov = r.rolling(window).cov(spy_ret)\n",
        "    var_spy = spy_ret.rolling(window).var()\n",
        "    risk[(sym,'BETA')] = cov / var_spy\n",
        "    risk[(sym,'SKEW')] = r.rolling(window).skew()\n",
        "\n",
        "risk_df = pd.DataFrame(risk)\n",
        "risk_df.columns.names = ['Symbol','Descriptor']\n",
        "price_d.to_csv('daily_prices_sp500.csv')\n",
        "returns.to_csv('monthly_returns_sp500.csv')\n",
        "risk_df.to_csv('risk_descriptors_sp500.csv')\n",
        "\n",
        "print(\"Done\")\n",
        "\n"
      ],
      "metadata": {
        "id": "c6DELuxwRXad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install innvestigate tensorflow"
      ],
      "metadata": {
        "id": "ud_5HVJVLcL7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PERIOD AND WINDOW SETTINGS\n",
        "START_PERIOD = pd.Period(\"2012-01\", \"M\")\n",
        "END_PERIOD   = pd.Period(\"2017-12\", \"M\")\n",
        "\n",
        "descriptor_window = 12\n",
        "TRAIN_MONTHS      = 12\n",
        "SEQ_LEN           = 5\n",
        "VAL_SPLIT         = 1/12\n",
        "\n",
        "sp500   = pd.read_csv(\"/content/sp500.csv\")\n",
        "tickers = sp500[\"Symbol\"].tolist()\n",
        "\n",
        "ret_df   = pd.read_csv(\"/content/monthly_returns_sp500.csv\",\n",
        "                       parse_dates=[\"Date\"], index_col=\"Date\")[tickers]\n",
        "raw_desc = (pd.read_csv(\"/content/risk_descriptors_sp500.csv\",\n",
        "                        header=[0, 1], index_col=0, parse_dates=True)[tickers])\n",
        "raw_desc.columns.names = [\"Ticker\", \"Descriptor\"]\n",
        "\n",
        "\n",
        "# align indices to periods and cut to common range\n",
        "ret_df.index, raw_desc.index = ret_df.index.to_period(\"M\"), raw_desc.index.to_period(\"M\")\n",
        "min_period = pd.Period(\"2010-01\", \"M\")\n",
        "ret_df, raw_desc = ret_df.loc[ret_df.index >= min_period], raw_desc.loc[raw_desc.index >= min_period]\n",
        "common = ret_df.index.intersection(raw_desc.index)\n",
        "ret_df, raw_desc = ret_df.loc[common], raw_desc.loc[common]\n",
        "\n",
        "\n",
        "# BUILD FEATURE AND TARGET ARRAYS\n",
        "descriptors = raw_desc.columns.get_level_values(\"Descriptor\").unique().tolist()\n",
        "N_FEAT = len(descriptors)\n",
        "\n",
        "dates, S = common, len(tickers)\n",
        "F = np.empty((len(dates), S, N_FEAT), dtype=float)\n",
        "for j, tkr in enumerate(tickers):\n",
        "    F[:, j, :] = raw_desc.xs(tkr, level=\"Ticker\", axis=1).values\n",
        "\n",
        "R = ret_df[tickers].values\n",
        "Y = np.roll(R, -1, axis=0)\n",
        "\n",
        "# DEFINE MODELS\n",
        "after_full = dates[descriptor_window:-1]\n",
        "after_idx  = [p for p in after_full if START_PERIOD <= p <= END_PERIOD]\n",
        "period_to_idx = {p: i for i, p in enumerate(dates)}\n",
        "\n",
        "lin_reg = make_pipeline(StandardScaler(), LinearRegression())\n",
        "svr     = make_pipeline(StandardScaler(), LinearSVR(max_iter=10000))\n",
        "rf      = RandomForestRegressor(n_estimators=50, max_depth=8, n_jobs=-1, random_state=0)\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "dnn  = Sequential([Input(shape=(SEQ_LEN * N_FEAT,)),\n",
        "                   Dense(40, activation=\"relu\"), Dense(1)])\n",
        "dnn.compile(\"adam\", \"mse\")\n",
        "\n",
        "lstm = Sequential([Input(shape=(SEQ_LEN, N_FEAT)),\n",
        "                   Bidirectional(LSTM(16)), Dense(1)])\n",
        "lstm.compile(\"adam\", \"mse\")\n",
        "\n",
        "\n",
        "# LRP SCORING FUNCTION\n",
        "def lrp_scores(model, X, is_lstm=False):\n",
        "    X_tf = tf.convert_to_tensor(X)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(X_tf)\n",
        "        y_pred = model(X_tf, training=False)\n",
        "    grads = tape.gradient(y_pred, X_tf)\n",
        "    gxi = tf.abs(grads * X_tf)\n",
        "    return (tf.reduce_sum(gxi, axis=[1, 2]) if is_lstm\n",
        "            else tf.reduce_sum(gxi, axis=1)).numpy()\n",
        "\n",
        "# SETUP METRIC CONTAINERS & EVAL PERIODS\n",
        "models = [\"LSTM\", \"LSTM+LRP\", \"DNN\", \"DNN+LRP\", \"Linear\", \"LinSVR\", \"RF\"]\n",
        "mae_hist, rmse_hist, port_hist = ({m: [] for m in models} for _ in range(3))\n",
        "\n",
        "\n",
        "#  Determine which months to evaluate over\n",
        "for today in tqdm(after_idx, desc=\"Eval 2012-2017\"):\n",
        "    i_today = period_to_idx[today]\n",
        "    train_idx = [i for i in range(i_today - TRAIN_MONTHS, i_today)\n",
        "                 if i >= descriptor_window]\n",
        "    if not train_idx:\n",
        "        continue\n",
        "\n",
        "    # BUILD TRAINING SET\n",
        "    Xtr_rows, ytr_rows = [], []\n",
        "    for i_d in train_idx:\n",
        "        xb = F[i_d - SEQ_LEN + 1:i_d + 1].transpose(1, 0, 2).reshape(S, SEQ_LEN * N_FEAT)\n",
        "        yb = Y[i_d]\n",
        "        mask = (~np.isnan(yb)) & (~np.isnan(xb).any(axis=1))\n",
        "        if mask.any():\n",
        "            Xtr_rows.append(xb[mask]); ytr_rows.append(yb[mask])\n",
        "    if not Xtr_rows:\n",
        "        continue\n",
        "    Xtr, ytr = np.vstack(Xtr_rows), np.hstack(ytr_rows)\n",
        "\n",
        "    # BUILD TEST SET\n",
        "    x_test = F[i_today - SEQ_LEN + 1:i_today + 1].transpose(1, 0, 2).reshape(S, SEQ_LEN * N_FEAT)\n",
        "    y_test = Y[i_today]\n",
        "    mask = (~np.isnan(y_test)) & (~np.isnan(x_test).any(axis=1))\n",
        "    if not mask.any():\n",
        "        continue\n",
        "    Xte, yte = x_test[mask], y_test[mask]\n",
        "    q = max(1, len(yte) // 5)\n",
        "\n",
        "    # FIT MODELS & PREDICT\n",
        "    p_lr  = lin_reg.fit(Xtr, ytr).predict(Xte)\n",
        "    p_svr = svr.fit(Xtr, ytr).predict(Xte)\n",
        "    rf.fit(Xtr, ytr); p_rf = rf.predict(Xte)\n",
        "\n",
        "    dnn.fit(Xtr, ytr, validation_split=VAL_SPLIT, epochs=4, batch_size=512,\n",
        "            verbose=0, callbacks=[EarlyStopping(patience=1,\n",
        "                                                restore_best_weights=True)])\n",
        "    p_dnn = dnn.predict(Xte, verbose=0).ravel()\n",
        "    r_dnn = lrp_scores(dnn, Xte)\n",
        "\n",
        "    Xtr_s, Xte_s = Xtr.reshape(-1, SEQ_LEN, N_FEAT), Xte.reshape(-1, SEQ_LEN, N_FEAT)\n",
        "    lstm.fit(Xtr_s, ytr, validation_split=VAL_SPLIT, epochs=4, batch_size=256,\n",
        "             verbose=0, callbacks=[EarlyStopping(patience=1,\n",
        "                                                 restore_best_weights=True)])\n",
        "    p_lstm = lstm.predict(Xte_s, verbose=0).ravel()\n",
        "    r_lstm = lrp_scores(lstm, Xte_s, is_lstm=True)\n",
        "\n",
        "    # Collect all predictions in a dict\n",
        "    preds = {\n",
        "        \"LSTM\": p_lstm,       \"LSTM+LRP\": r_lstm,\n",
        "        \"DNN\":  p_dnn,        \"DNN+LRP\":  r_dnn,\n",
        "        \"Linear\": p_lr,       \"LinSVR\":   p_svr,\n",
        "        \"RF\": p_rf,\n",
        "    }\n",
        "\n",
        "\n",
        "    # RECORD METRICS & PORTFOLIO RETURNS\n",
        "    for name, pred in preds.items():\n",
        "        if name in (\"LSTM\", \"DNN\", \"Linear\", \"LinSVR\", \"RF\"):\n",
        "            mae_hist[name].append(mean_absolute_error(yte, pred))\n",
        "            rmse_hist[name].append(np.sqrt(mean_squared_error(yte, pred)))\n",
        "        else:\n",
        "            mae_hist[name].append(np.nan); rmse_hist[name].append(np.nan)\n",
        "\n",
        "        idx = np.argsort(pred); longs, shorts = idx[-q:], idx[:q]\n",
        "        port_hist[name].append(yte[longs].mean() - yte[shorts].mean())\n",
        "\n",
        "# AGGREGATE RESULTS INTO SUMMARY TABLE\n",
        "rows = []\n",
        "for name in models:\n",
        "    mae_vals  = np.asarray(mae_hist[name],  dtype=float)\n",
        "    rmse_vals = np.asarray(rmse_hist[name], dtype=float)\n",
        "\n",
        "    # Compute average MAE/RMSE\n",
        "    mae  = np.nanmean(mae_vals)  if np.isfinite(mae_vals).any()  else np.nan\n",
        "    rmse = np.nanmean(rmse_vals) if np.isfinite(rmse_vals).any() else np.nan\n",
        "\n",
        "\n",
        "    # Compute annualized return, volatility, and Sharpe ratio\n",
        "    rets = np.asarray(port_hist[name], dtype=float)\n",
        "    if rets.size:\n",
        "        ann_rt = (1 + rets).prod()**(12 / rets.size) - 1\n",
        "        ann_vl = rets.std(ddof=0) * np.sqrt(12)\n",
        "        sharpe = ann_rt / ann_vl if ann_vl else np.nan\n",
        "    else:\n",
        "        ann_rt = ann_vl = sharpe = np.nan\n",
        "\n",
        "    rows.append(dict(Model=name, Return=ann_rt * 100, Vol=ann_vl * 100,\n",
        "                     Sharpe=sharpe, MAE=mae, RMSE=rmse))\n",
        "\n",
        "# Build and format the final summary DataFrame, then print\n",
        "summary = (pd.DataFrame(rows)\n",
        "             .set_index(\"Model\")\n",
        "             .rename(columns={\"Return\":\"Return[%]\", \"Vol\":\"Vol[%]\"}))\n",
        "\n",
        "col_blocks = [\n",
        "    (\"Deep recurrent factor model\", \"LSTM\"),\n",
        "    (\"Deep recurrent factor model\", \"LSTM+LRP\"),\n",
        "    (\"Deep factor model\",          \"DNN\"),\n",
        "    (\"Deep factor model\",          \"DNN+LRP\"),\n",
        "    (\"Linear model\",               \"Linear\"),\n",
        "    (\"SVR\",                        \"LinSVR\"),\n",
        "    (\"Random forest\",              \"RF\"),\n",
        "]\n",
        "summary = summary.reindex(columns=[\"Return[%]\", \"Vol[%]\", \"Sharpe\", \"MAE\", \"RMSE\"])\n",
        "summary = summary.T.reindex([m for _, m in col_blocks], axis=1)\n",
        "summary.columns = pd.MultiIndex.from_tuples(col_blocks,\n",
        "                                            names=[\"Family\", \"Method\"])\n",
        "\n",
        "fmt = {k: \"{:.2f}\" for k in [\"Return[%]\", \"Vol[%]\", \"Sharpe\"]}\n",
        "fmt.update({k: \"{:.5f}\" for k in [\"MAE\", \"RMSE\"]})\n",
        "\n",
        "pretty = summary.copy()\n",
        "for row in pretty.index:\n",
        "    pretty.loc[row] = pretty.loc[row].apply(\n",
        "        lambda v, f=fmt[row]: f.format(v) if np.isfinite(v) else \"—\"\n",
        "    )\n",
        "\n",
        "print(\"\\nSummary (2012-2017)\")\n",
        "print(pretty)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrXOEdrCZoTO",
        "outputId": "980ec9db-9c5f-415a-930f-8b3895770e56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eval 2012-2017: 100%|██████████| 72/72 [08:14<00:00,  6.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary (2012-2017)\n",
            "Family    Deep recurrent factor model          Deep factor model          \\\n",
            "Method                           LSTM LSTM+LRP               DNN DNN+LRP   \n",
            "Return[%]                        5.70     5.79             -1.52    3.37   \n",
            "Vol[%]                           9.00    13.33              6.41   10.49   \n",
            "Sharpe                           0.63     0.43             -0.24    0.32   \n",
            "MAE                           0.05011        —           0.05883       —   \n",
            "RMSE                          0.06706        —           0.08032       —   \n",
            "\n",
            "Family    Linear model      SVR Random forest  \n",
            "Method          Linear   LinSVR            RF  \n",
            "Return[%]         2.50     2.02          2.08  \n",
            "Vol[%]            8.79     7.79          9.35  \n",
            "Sharpe            0.28     0.26          0.22  \n",
            "MAE            0.05073  0.05088       0.05050  \n",
            "RMSE           0.06771  0.06791       0.06782  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}